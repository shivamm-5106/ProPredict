{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading ESM2-150M model...\n",
      "Model loaded successfully!\n",
      "✅ Loaded 1 cached embeddings from embeddings_150M.pkl\n",
      "ℹ️ Found cached embedding for seq_001\n",
      "\n",
      "Embedding shape: torch.Size([640])\n",
      "First 10 elements: tensor([-0.0012, -0.0170,  0.0506, -0.0911, -0.0596, -0.0686, -0.1527,  0.0025,\n",
      "         0.0370,  0.1412])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import esm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model and alphabet\n",
    "print(\"Loading ESM2-150M model...\")\n",
    "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Path to embeddings file\n",
    "EMB_PATH = \"embeddings_150M.pkl\"\n",
    "\n",
    "# Load existing embeddings if available\n",
    "if os.path.exists(EMB_PATH):\n",
    "    with open(EMB_PATH, \"rb\") as f:\n",
    "        embedding_dict = pickle.load(f)\n",
    "    print(f\"✅ Loaded {len(embedding_dict)} cached embeddings from {EMB_PATH}\")\n",
    "else:\n",
    "    embedding_dict = {}\n",
    "    print(\"⚠️ No existing embeddings found — starting fresh\")\n",
    "\n",
    "def get_embedding(sequence: str, seq_id: str):\n",
    "    \"\"\"Return embedding for a protein sequence, with caching.\"\"\"\n",
    "    if seq_id in embedding_dict:\n",
    "        print(f\"ℹ️ Found cached embedding for {seq_id}\")\n",
    "        return embedding_dict[seq_id]\n",
    "\n",
    "    # Prepare batch\n",
    "    data = [(seq_id, sequence)]\n",
    "    _, _, batch_tokens = batch_converter(data)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[30])\n",
    "        token_representations = results[\"representations\"][30]\n",
    "\n",
    "    # Mean pooling\n",
    "    embedding = token_representations.mean(1).squeeze().cpu()\n",
    "\n",
    "    # Save to cache\n",
    "    embedding_dict[seq_id] = embedding\n",
    "    with open(EMB_PATH, \"wb\") as f:\n",
    "        pickle.dump(embedding_dict, f)\n",
    "\n",
    "    print(f\"✅ Generated and cached embedding for {seq_id}\")\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example sequence\n",
    "    seq_id = \"seq_001\"\n",
    "    seq = \"MKTFFVLVLLLAAAGVAGTQATQGNVKAAW\"\n",
    "\n",
    "    # If embeddings already exist for this seq, it will skip recomputation\n",
    "    emb = get_embedding(seq, seq_id)\n",
    "\n",
    "    print(f\"\\nEmbedding shape: {emb.shape}\")\n",
    "    print(f\"First 10 elements: {emb[:10]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
